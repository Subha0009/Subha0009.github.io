<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Subhabrata Dutta - Senior Research Associate, IIT Delhi</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }

        header {
            background-color: #333;
            color: #fff;
            text-align: left;
            padding: 1em 0;
            display: flex;
            justify-content: space-between;
            max-width: 800px;
            margin: 0 auto;
        }
        header img {
            max-width: 100px;
            height: auto;
            margin-right: 20px;
            margin-left: 10px;
            border-radius: 50%;
            overflow: hidden;
        }
        header div {
            margin-right: 20px;
        }
        h1, p {
            margin: 0;
        }

        nav {
            background-color: #444;
            color: #fff;
            text-align: center;
            padding: 0.5em 0;
            max-width: 800px;
            margin: 0 auto;
        }

        nav a {
            color: #fff;
            text-decoration: none;
            padding: 1em;
        }

        section {
            padding: 2em;
            text-align: justify;
            max-width: 800px;
            margin: 0 auto;
        }

        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 1em 0;
            max-width: 800px; /* Set your desired maximum width */
            margin: 0 auto;
        }
    </style>
</head>
<body>

    <header>
        <img src="profile.jpg" alt="Subhabrata Dutta">
        <div>
            <h1>Subhabrata Dutta</h1>
            <p>Senior Research Associate, IIT Delhi</p>
        </div>
    </header>

    <nav>
        <a href="#home">Home</a>
        <a href="#research">Research</a>
        <a href="#publications">Publications</a>
        <a href="#contact">Contact</a>
    </nav>

    <section id="home">
        <p>I am an NLP researcher with broad interest in Machine Learning in general. Currently, I am working as a Senior Research Associate at <a href="https://www.lcs2.in/" target="_blank">Laboratory for Computational Social Systems, IIT Delhi</a>, mentored by <a href="https://www.tanmoychak.com/" target="_blank">Dr. Tanmoy Chakraborty</a> and <a href="https://www.cse.iitb.ac.in/~soumen/" target="_blank">Prof. Soumen Chakrabarti</a>. My current research interest revolves around Large Language Models; precisely focused on reasoning, prompt engineering, and, interpretation. Additionally, I share interest in Temporal Graph Representation Learning. I submitted my doctoral thesis, titled <i>Engagement to Persuasion: A Computational Study on Online Social Discourse</i>, in 2023. My doctoral research is centered around the qualitative and quantitative analysis of online social platforms.</p>
    </section>

    <section id="research">
        <h2>Research Overview</h2>
        <h3>Large Language Models</h3>
        <p> Reasoning with LLMs is one of my key research interests. I have been exploring different techniques to ellicit superior mathematical reasoning capabilities into relativelt smaller models, such as, separation and finetuning of problem decomposition expertise for modular reasoning <a href="https://arxiv.org/pdf/2310.18338.pdf" target="_blank">[EMNLP 2023]</a>, reinforcement learning from tool-usage feedbacks, etc. I am currently working on mechnistic interpretation of LLM reasoning and knowledge retrieval. Additionally, I have been working on prompt engineering in low-resource settings. My work on cross-lingual In-context learning has received outstanding paper award at <a href="https://aclanthology.org/2023.acl-long.346.pdf" target="_blank">[ACL 2023]</a>. In my doctoral research, I have worked on aligning pretrained LMs with unsupervised finetuning towards superior argument understanding <a href="https://aclanthology.org/2022.acl-long.536.pdf" target="_blank">[ACL 2022]</a>. Earlier, I have explored the possibilities of building compute-efficient Transformer architectures from the perspective of dynamical systems <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/2bd388f731f26312bfc0fe30da009595-Paper.pdf" target="_blank">[NeurIPS 2021]</a>.</p>
        <h3> Social discussion mining</h3>
        <p>In my doctoal research, I worked on predictive modeling of user engagement in online platforms under various exogenous and endogenous influences <a href="https://ieeexplore.ieee.org/abstract/document/9863674" target="_blank">[TKDE 2022]</a><a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403251" target="_blank">[KDD 2020]</a>. An improtant problem of my doctoral thesis has been determination of the interdependence between user opinions and network dynamics <a href="https://dl.acm.org/doi/abs/10.1145/3488560.3498511" target="_blank">[WSDM 2022]</a><a href="https://academic.oup.com/pnasnexus/article/2/3/pgad041/7031416" target="_blank">[PNAS Nexus 2023]</a>.</p>
        <h3>Temporal network representation learning</h3>
        <p>Primarily stemmed from my doctoral research, I have been working with representation learning of temporal graphs and interaction networks, including inductive link prediction, incremental learning on large graphs, and, geometric deep learning.</p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <p>Check out my <a href="https://scholar.google.com/citations?user=aoaCs08AAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> for an extensive list of publications.</p>
        <ul>
            <li>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning<a href="https://arxiv.org/pdf/2310.18338.pdf" target="_blank">[pdf]</a><a href="https://github.com/LCS2-IIITD/DaSLaM" target="_blank">[code]</a></li>
            <p>Reasoning with LLMs can be done better if we separate out the solver (typically a larger model) from the decomposer (typically a smaller model) and finetune the latter to break down the problem upon feedback from the solver. Decomposer is finetuned as an agent interacting with the blackbox environment constructed by the solver; that is, once a decomposer is trained, it can act with any solver of any scale.</p>
            <li>Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment<a href="https://aclanthology.org/2023.acl-long.346.pdf" target="_blank">[pdf]</a><a href="https://github.com/EshaanT/X-InSTA" target="_blank">[code]</a></li>
            <p>Almost random predictions in cross-lingual ICL from multilingual LLMs can be made significantly better via alignment: choose examples semantically similar to the target input, and transfer the task knowledge from source language to target using manually designed aligners.</p>
            <li>Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?<a href="https://aclanthology.org/2022.acl-long.536.pdf" target="_blank">[pdf]</a><a href="https://github.com/Jeevesh8/arg_mining" target="_blank">[code]</a></li>
            
        </ul>
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>Provide contact information or a contact form for visitors to get in touch with you.</p>
    </section>

    <footer>
        <p>&copy; 2023 Subhabrata Dutta. All rights reserved.</p>
    </footer>

</body>
</html>

