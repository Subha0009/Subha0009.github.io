<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Subhabrata Dutta, IIT Delhi</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }

        header {
            background-color: #333;
            color: #fff;
            text-align: left;
            padding: 1em 0;
            display: flex;
            justify-content: space-between;
            max-width: 800px;
            margin: 0 auto;
        }
        header img {
            max-width: 100px;
            height: auto;
            margin-right: 20px;
            margin-left: 10px;
            border-radius: 50%;
            overflow: hidden;
        }
        header div {
            margin-right: 20px;
        }
        h1, p {
            margin: 0;
        }

        nav {
            background-color: #444;
            color: #fff;
            text-align: center;
            padding: 0.5em 0;
            max-width: 800px;
            margin: 0 auto;
        }

        nav a {
            color: #fff;
            text-decoration: none;
            padding: 1em;
        }

        section {
            padding: 2em;
            text-align: justify;
            max-width: 800px;
            margin: 0 auto;
        }

        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 1em 0;
            max-width: 800px; /* Set your desired maximum width */
            margin: 0 auto;
        }
    </style>
</head>
<body>

    <header>
        <img src="profile.jpg" alt="Subhabrata Dutta">
        <div>
            <h1>Subhabrata Dutta</h1>
            <p>Senior Research Associate, IIT Delhi</p>
        </div>
    </header>

    <nav>
        <a href="#home">Home</a>
        <a href="#research">Research</a>
        <a href="#publications">Publications</a>
        <a href="#cv">CV</a>
        <a href="#research interests">Research Interests</a>
        <a href="#contact">Contact</a>
    </nav>

    <section id="home">
        <p>I am an NLP researcher with broad interest in Machine Learning in general. Currently, I am working as a Senior Research Associate at <a href="https://www.lcs2.in/" target="_blank">Laboratory for Computational Social Systems, IIT Delhi</a>, mentored by <a href="https://www.tanmoychak.com/" target="_blank">Dr. Tanmoy Chakraborty</a> and <a href="https://www.cse.iitb.ac.in/~soumen/" target="_blank">Prof. Soumen Chakrabarti</a>. My current research interest revolves around Large Language Models; precisely focused on reasoning, prompt engineering, and, interpretation. Additionally, I share interest in Temporal Graph Representation Learning. I completed my PhD in 2023 with my doctoral thesis titled <i>Engagement to Persuasion: A Computational Study on Online Social Discourse</i>. My doctoral research is centered around the qualitative and quantitative analysis of online social platforms.</p>
    </section>

    <section id="research">
        <h2>Research</h2>
        <h3>Large Language Models</h3>
        <p> Reasoning with LLMs is one of my key research interests. I have been exploring different techniques to elicit superior mathematical reasoning capabilities into relatively smaller models, such as, separation and finetuning of problem decomposition expertise for modular reasoning <a href="https://arxiv.org/pdf/2310.18338.pdf" target="_blank">[EMNLP 2023]</a>, reinforcement learning from tool-usage feedback <a href="https://arxiv.org/pdf/2312.05571.pdf" target="_blank">[AAAI 2024]</a>, etc. I am currently working on mechanistic interpretation of LLM reasoning and knowledge retrieval <a href="https://arxiv.org/abs/2402.18312" target="_blank">[Preprint]</a>. Additionally, I have been working on nuances of in-context learning in low-resource settings. My work on cross-lingual In-context learning has received outstanding paper award at <a href="https://aclanthology.org/2023.acl-long.346.pdf" target="_blank">[ACL 2023]</a>. You may check out my recent <a href="https://dl.acm.org/doi/pdf/10.1145/3616863" target="_blank">opinion piece on reliability of AI assistants for science communications</a>, published in the <i>Communications of The ACM</i>. In my doctoral research, I have worked on aligning pretrained LMs with unsupervised finetuning towards superior argument understanding <a href="https://aclanthology.org/2022.acl-long.536.pdf" target="_blank">[ACL 2022]</a>. Earlier, I have explored the possibilities of building compute-efficient Transformer architectures from the perspective of dynamical systems <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/2bd388f731f26312bfc0fe30da009595-Paper.pdf" target="_blank">[NeurIPS 2021]</a>.</p>
        <h3> Social discussion mining</h3>
        <p>In my doctoral research, I worked on predictive modeling of user engagement in online platforms under various exogenous and endogenous influences <a href="https://ieeexplore.ieee.org/abstract/document/9863674" target="_blank">[TKDE 2022]</a><a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403251" target="_blank">[KDD 2020]</a>. An important problem explored in my doctoral thesis was determination of the interdependence between user opinions and network dynamics <a href="https://dl.acm.org/doi/abs/10.1145/3488560.3498511" target="_blank">[WSDM 2022]</a><a href="https://academic.oup.com/pnasnexus/article/2/3/pgad041/7031416" target="_blank">[PNAS Nexus 2023]</a>.</p>
        <h3>Temporal network representation learning</h3>
        <p>Primarily stemmed from my doctoral research, I have been working with representation learning of temporal graphs and interaction networks, including inductive link prediction, incremental learning on large graphs, and, geometric deep learning.</p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <p>Check out my <a href="https://scholar.google.com/citations?user=aoaCs08AAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> for an extensive list of publications.</p>
        <ul>
            <li><i>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</i><a href="https://arxiv.org/abs/2402.18312" target="_blank">[pdf]</a><a href="https://github.com/joykirat18/how-to-think-step-by-step" target="_blank">[code]</a></li>
            <li><i>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</i><a href="https://arxiv.org/pdf/2310.18338.pdf" target="_blank">[pdf]</a><a href="https://github.com/LCS2-IIITD/DaSLaM" target="_blank">[code]</a></li>
            <p>Reasoning with LLMs can be done better if we separate out the solver (typically a larger model) from the decomposer (typically a smaller model) and finetune the latter to break down the problem upon feedback from the solver. Decomposer is finetuned as an agent interacting with the blackbox environment constructed by the solver; that is, once a decomposer is trained, it can act with any solver of any scale.</p>
            <li><i>Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment</i><a href="https://aclanthology.org/2023.acl-long.346.pdf" target="_blank">[pdf]</a><a href="https://github.com/EshaanT/X-InSTA" target="_blank">[code]</a></li>
            <p>Almost random predictions in cross-lingual ICL from multilingual LLMs can be made significantly better via alignment: choose examples semantically similar to the target input, and transfer the task knowledge from source language to target using manually designed aligners.</p>
            <li><i>Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?</i><a href="https://aclanthology.org/2022.acl-long.536.pdf" target="_blank">[pdf]</a><a href="https://github.com/Jeevesh8/arg_mining" target="_blank">[code]</a></li>
            <p>Pretrained LMs can be made aware of the signals of argumentative discourse via an unsupervised finetuning step using social discussion data. Additionally, prompt-based finetuning that are aligned to the pretraining objective can elicit generalizable argument understanding.</p>
            <li><i>Redesigning the transformer architecture with insights from multi-particle dynamical systems</i><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/2bd388f731f26312bfc0fe30da009595-Paper.pdf" target="_blank">[pdf]</a><a href="https://github.com/LCS2-IIITD/TransEvolve" target="_blank">[code]</a></li>
            <p>Transformer architecture follows a close analogy with the temporal evolution of a multi-particle dynamical system. By incorporating explicit depth-wise evolution operator, one can come up with compute- as well as parameter-efficient sequence-to-sequence architectures that are as good as the original transformer if not better.</p>
        </ul>
    </section>

    <section id="cv">
        <h2>CV</h2>
        <p><a href="Subhabrata%20CV.pdf" target="_blank" download>Download CV</a> 
    </section>

    <section id="research interests">
        <h2>Research Interests</h2>
        <p><a href="Research%20interest.pdf" target="_blank" download>Download Statement of Research Inetersts</a> 
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>You can connect me via subha0009 [at] gmail [dot] com</p>
    </section>

    <footer>
        <p>&copy; 2023 Subhabrata Dutta. All rights reserved.</p>
    </footer>

</body>
</html>

